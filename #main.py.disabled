from DrissionPage import ChromiumPage, ChromiumOptions
from datetime import datetime, timedelta
import re
import os
import time
import tempfile
import shutil
import csv

# --- é…ç½®éƒ¨åˆ† ---
KEYWORDS =  [ "å¹¿ä¸œä½“è‚²", "æ— çº¿æ–°é—»", "ç¿¡ç¿ å°", "VIU", "TVB PLUS", "Now Sports ç²¾é¸"]
DAYS_LIMIT = 30
DATA_FILE = "data.csv"
M3U_FILE = "tv.m3u"
TXT_FILE = "tv.txt"

def handle_cloudflare(page):
    """(ä¿®æ”¹ç‰ˆ) å°è¯•å¤„ç† Cloudflare å’Œ reCAPTCHA éªŒè¯"""
    print("ğŸ›¡ï¸ Checking Cloudflare/reCAPTCHA status...")
    
    # å¢åŠ ç­‰å¾…è½®æ¬¡ï¼Œç»™éªŒè¯ç•™å‡ºæ—¶é—´
    for i in range(20): 
        time.sleep(2)
        
        try:
            title = page.title
            # 1. æˆåŠŸåˆ¤æ–­ï¼šæ ‡é¢˜æ­£å¸¸ï¼Œä¸”é¡µé¢é‡Œæ²¡æœ‰éªŒè¯ç çš„ç‰¹å¾æ–‡å­—
            if "Just a moment" not in title and "Not a Robot" not in page.html and ("IPTV" in title or "Search" in title or "Tonkiang" in title):
                print(f"âœ… Access Granted! (Title: {title})")
                return True
            
            # 2. å°è¯•å®šä½å¹¶ç‚¹å‡» reCAPTCHA å¤é€‰æ¡†
            # Google çš„éªŒè¯ç é€šå¸¸åœ¨ä¸€ä¸ª iframe é‡Œï¼Œsrc åŒ…å« google.com/recaptcha
            recaptcha_iframe = page.get_frame('@src^https://www.google.com/recaptcha/api2/anchor')
            if recaptcha_iframe:
                # æŸ¥æ‰¾é‚£ä¸ªå°æ–¹æ¡†
                checkbox = recaptcha_iframe.ele('#recaptcha-anchor')
                # å¦‚æœæ²¡è¢«å‹¾é€‰ï¼Œå°±ç‚¹ä¸€ä¸‹
                if checkbox and 'recaptcha-checkbox-checked' not in checkbox.attr('class'):
                    print("ğŸ¤– Found reCAPTCHA, clicking checkbox...")
                    checkbox.click()
                    time.sleep(3) # ç­‰å¾…å˜ç»¿æˆ–è€…å¼¹å‡ºå›¾ç‰‡
            
            # 3. å°è¯•ç‚¹å‡»æˆªå›¾é‡Œé‚£ä¸ª "OK" æŒ‰é’®
            # æˆªå›¾æ˜¾ç¤ºæœ‰ä¸€ä¸ªå·¨å¤§çš„ "OK" æŒ‰é’®ï¼Œå¯èƒ½éœ€è¦ç‚¹å®ŒéªŒè¯ç å†ç‚¹å®ƒï¼Œæˆ–è€…ç›´æ¥ç‚¹å®ƒ
            ok_btn = page.ele('tag:button@@text()=OK') or page.ele('tag:input@@value=OK') or page.ele('text:^OK$')
            if ok_btn:
                print("ğŸ‘† Found OK button, clicking...")
                ok_btn.click()
                
        except Exception as e:
            # åªæ˜¯å°è¯•ï¼ŒæŠ¥é”™äº†ä¸è¦ä¸­æ–­ï¼Œç»§ç»­ä¸‹ä¸€è½®å¾ªç¯æ£€æµ‹
            pass

    print("âš ï¸ Cloudflare/reCAPTCHA check timed out")
    return False

# --- è¯»å–å†å² CSV ---
def load_history():
    history = {}
    if os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, 'r', encoding='utf-8', newline='') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    history[row['URL']] = {
                        'Channel': row['Channel'],
                        'Date': row['Date'],
                        'Keyword': row['Keyword']
                    }
            print(f"ğŸ“– Loaded {len(history)} items from history.")
        except: pass
    return history

# --- ä¿å­˜æ‰€æœ‰æ–‡ä»¶ ---
def save_files(data_dict):
    try:
        # 1. ä¿å­˜ CSV
        with open(DATA_FILE, 'w', encoding='utf-8', newline='') as f:
            fieldnames = ['Keyword', 'Channel', 'Date', 'URL']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            sorted_items = sorted(data_dict.items(), key=lambda x: x[1]['Keyword'])
            for url, info in sorted_items:
                writer.writerow({'Keyword': info['Keyword'], 'Channel': info['Channel'], 'Date': info['Date'], 'URL': url})
        
        # 2. ä¿å­˜ M3U
        with open(M3U_FILE, 'w', encoding='utf-8') as f:
            f.write("#EXTM3U\n")
            for url, info in data_dict.items():
                f.write(f'#EXTINF:-1 group-title="{info["Keyword"]}",{info["Channel"]}\n{url}\n')

        # 3. ä¿å­˜ TXT
        with open(TXT_FILE, 'w', encoding='utf-8') as f:
            for url, info in data_dict.items():
                f.write(f'{info["Channel"]},{url}\n')

        print(f"ğŸ’¾ Database updated: {len(data_dict)} items saved.")
    except Exception as e:
        print(f"âŒ Save failed: {e}")

def main():
    # --- 1. ç¯å¢ƒé…ç½® ---
    temp_user_dir = tempfile.mkdtemp()
    co = ChromiumOptions()
    co.headless(True)
    co.set_argument('--no-sandbox')
    co.set_argument('--disable-gpu')
    co.set_argument('--disable-dev-shm-usage')
    co.set_argument('--window-size=1920,1080')
    co.set_argument(f'--user-data-dir={temp_user_dir}')
    co.set_argument('--remote-allow-origins=*')
    co.set_user_agent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36')

    chrome_path = os.getenv('CHROME_PATH')
    if chrome_path:
        co.set_paths(browser_path=chrome_path)

    try:
        page = ChromiumPage(co)
        print("âœ… Browser launched successfully!")
    except Exception as e:
        print(f"âŒ Browser Init Failed: {e}")
        try: shutil.rmtree(temp_user_dir) 
        except: pass
        return

    # --- 2. å‡†å¤‡æ•°æ® ---
    all_data = load_history()
    current_date = datetime.now()
    cutoff_date = current_date - timedelta(days=DAYS_LIMIT)

    try:
        # --- 3. å¾ªç¯æœç´¢ ---
        for kw in KEYWORDS:
            print(f"\nğŸš€ Processing Keyword: {kw}")
            
            try:
                page.get('http://tonkiang.us/')
                handle_cloudflare(page) 
                
                search_input = page.ele('tag:input@@type!=hidden', timeout=5)
                if search_input:
                    search_input.clear()
                    search_input.input(kw)
                    
                    # æäº¤æœç´¢
                    try:
                        search_btn = search_input.next('tag:button') or page.ele('tag:button@@type=submit')
                        if search_btn:
                            search_btn.click()
                        else:
                            search_input.input('\n')
                    except:
                        search_input.input('\n')
                    
                    page.wait(3) 

                else:
                    print(f"âŒ Input box not found for {kw}, skipping.")
                    continue

                # --- 4. é€šç”¨æå–é€»è¾‘ (ä¸å†é’ˆå¯¹ç‰¹å®šè¯) ---
                items = page.eles('text:://')
                new_found = 0
                
                for item in items:
                    try:
                        # 1. æå– URL
                        txt = item.text
                        url_match = re.search(r'((?:http|https|rtmp|rtsp)://[^\s<>"\u4e00-\u9fa5]+)', txt)
                        if not url_match: continue
                        url = url_match.group(1)

                        # 2. å¯»æ‰¾å®Œæ•´ä¿¡æ¯å—
                        container = item
                        full_text_block = ""
                        
                        # å‘ä¸Šæ‰¾åŒ…å«æ¢è¡Œç¬¦çš„çˆ¶çº§ï¼Œè¿™æ˜¯æœ€å‡†ç¡®çš„å®šä½æ–¹å¼
                        for _ in range(3):
                            container = container.parent()
                            if not container: break
                            if "\n" in container.text:
                                full_text_block = container.text
                                break
                        
                        if not full_text_block:
                            full_text_block = container.text if container else ""

                        # 3. æŒ‰è¡Œè§£æ (é€šç”¨é€»è¾‘)
                        # æ¸…æ´—æ¯ä¸€è¡Œï¼šå»æ‰é¦–å°¾ç©ºæ ¼ã€å»æ‰åˆ¶è¡¨ç¬¦ã€å»æ‰çœ‹ä¸è§çš„ç¬¦å·
                        lines = [line.strip() for line in full_text_block.split('\n') if line.strip()]
                        
                        channel_name = "" # åˆå§‹ä¸ºç©ºï¼Œä¸é¢„è®¾ä¸º kw
                        date_str = ""
                        
                        for line in lines:
                            # å¿½ç•¥ URL è¡Œ
                            if "://" in line: continue
                            
                            # æ£€æŸ¥æ˜¯å¦æ˜¯æ—¥æœŸè¡Œ
                            mat = re.search(r'(\d{2,4}-\d{1,2}-\d{2,4})', line)
                            if mat:
                                date_str = mat.group(1)
                                continue 
                            
                            # å¦‚æœè¿˜æ²¡æ‰¾åˆ°å°åï¼Œä¸”è¿™è¡Œä¸æ˜¯URLä¹Ÿä¸æ˜¯æ—¥æœŸï¼Œé‚£å®ƒå°±æ˜¯å°å
                            # è¿™é‡Œä¸å†æ£€æŸ¥ len(line) < 50ï¼Œé˜²æ­¢æŸäº›é•¿åå­—è¢«æ¼æ‰
                            # ä¹Ÿä¸å†æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®å­—ï¼Œå®Œå…¨ä¿¡ä»»é¡µé¢æ˜¾ç¤º
                            if not channel_name:
                                channel_name = line
                        
                        # å¦‚æœå®åœ¨æ²¡æå–åˆ°å°åï¼Œæ‰ç”¨å…³é”®å­—å…œåº• (é˜²æ­¢ç©ºå)
                        if not channel_name:
                            channel_name = kw

                        # 4. å­˜å…¥æ•°æ®
                        if date_str:
                            try:
                                if len(date_str.split('-')[0]) == 4:
                                    dt = datetime.strptime(date_str, '%Y-%m-%d')
                                else:
                                    dt = datetime.strptime(date_str, '%m-%d-%Y')
                                str_date = dt.strftime('%Y-%m-%d')

                                # æ ¸å¿ƒï¼šæ€»æ˜¯ç”¨é¡µé¢ä¸ŠæŠ“åˆ°çš„çœŸå®åå­— (channel_name) æ›´æ–°æ•°æ®åº“
                                if url in all_data:
                                    # å³ä½¿ URL å·²å­˜åœ¨ï¼Œåªè¦é¡µé¢ä¸Šçš„åå­—ä¸æ˜¯é»˜è®¤å…³é”®å­—ï¼Œå°±æ›´æ–°å®ƒ
                                    # è¿™æ ·å¯ä»¥ä¿®æ­£ä»¥å‰è¢«é”™è¯¯å­˜ä¸º "VIU" çš„æ•°æ®
                                    if channel_name != kw:
                                        all_data[url]['Channel'] = channel_name
                                    
                                    # æ›´æ–°æ—¥æœŸ
                                    old_date = datetime.strptime(all_data[url]['Date'], '%Y-%m-%d')
                                    if dt > old_date:
                                        all_data[url]['Date'] = str_date
                                else:
                                    # æ–°å¢
                                    all_data[url] = {
                                        'Keyword': kw,
                                        'Channel': channel_name,
                                        'Date': str_date
                                    }
                                    new_found += 1
                            except: pass
                    except: continue
                
                print(f"   - {kw}: Processed. Found {new_found} new items.")

            except Exception as e:
                print(f"âŒ Error scraping {kw}: {e}")
                continue

    except Exception as e:
        print(f"âŒ Global Error: {e}")
    finally:
        if page: page.quit()
        try: shutil.rmtree(temp_user_dir)
        except: pass

    # --- 4. æ¸…ç†ä¸ä¿å­˜ ---
    print("\nğŸ§¹ Cleaning old data...")
    valid_data = {}
    expired_count = 0
    
    for url, info in all_data.items():
        try:
            item_date = datetime.strptime(info['Date'], '%Y-%m-%d')
            if item_date >= cutoff_date:
                valid_data[url] = info
            else:
                expired_count += 1
        except:
            expired_count += 1

    print(f"   Removed {expired_count} expired items.")
    
    if len(valid_data) > 0:
        save_files(valid_data)
    else:
        print("âš ï¸ No valid data remaining! Skipping save.")

if __name__ == "__main__":
    main()

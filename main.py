from DrissionPage import ChromiumPage, ChromiumOptions
from datetime import datetime, timedelta
import re
import os
import time
import tempfile
import shutil

def handle_cloudflare(page):
    """
    æ™ºèƒ½å¤„ç† Cloudflareï¼Œæ£€æµ‹åˆ°å·²è¿›å…¥é¦–é¡µåˆ™ç«‹å³æ”¾è¡Œ
    """
    print("ğŸ›¡ï¸ Checking Cloudflare status...")
    
    for i in range(10):
        try:
            title = page.title
            # å¦‚æœæ ‡é¢˜åŒ…å« Tonkiang çš„ç‰¹å¾è¯ï¼Œæˆ–è€… IPTV Searchï¼Œè¯´æ˜å·²ç»è¿›å»äº†
            if "Just a moment" not in title and ("IPTV" in title or "Search" in title or "Tonkiang" in title):
                print(f"âœ… Access Granted! (Title: {title})")
                return True
            
            print(f"   - Still in waiting room... ({i+1}/10)")
            time.sleep(3)
        except:
            time.sleep(3)
    
    print("âš ï¸ Cloudflare check timed out (trying to proceed anyway)")
    return False

def main():
    # --- 1. ç¯å¢ƒé…ç½® ---
    temp_user_dir = tempfile.mkdtemp()
    co = ChromiumOptions()
    co.headless(True)
    co.set_argument('--no-sandbox')
    co.set_argument('--disable-gpu')
    co.set_argument('--disable-dev-shm-usage')
    co.set_argument(f'--user-data-dir={temp_user_dir}')
    co.set_argument('--remote-allow-origins=*')
    co.set_user_agent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36')

    chrome_path = os.getenv('CHROME_PATH')
    if chrome_path:
        co.set_paths(browser_path=chrome_path)

    try:
        page = ChromiumPage(co)
        print("âœ… Browser launched successfully!")
    except Exception as e:
        print(f"âŒ Browser Init Failed: {e}")
        try: shutil.rmtree(temp_user_dir) 
        except: pass
        return

    # --- 2. é‡‡é›†é…ç½® ---
    keywords = ["æ— çº¿æ–°é—»", "å¹¿ä¸œä½“è‚²", "ç¿¡ç¿ å°", "VIU", "tvb plus", "now SPORTS PRIME", "Now Sports ç²¾é¸", "TLC å°æ¹¾", "Discovery", "åœ‹å®¶åœ°ç†", "NatGeo", "HBO"]
    days_limit = 30  # æ¢å¤åˆ° 30 å¤©ï¼Œè¿‡æ»¤é™ˆæ—§æº
    time_threshold = datetime.now() - timedelta(days=days_limit)
    
    # ç”¨äºå­˜å‚¨æœ€ç»ˆç»“æœ
    final_results = []
    # ç”¨äºå»é‡ï¼Œé˜²æ­¢åŒä¸€ä¸ª URL å‡ºç°å¤šæ¬¡
    seen_urls = set()

    try:
        # --- 3. å¾ªç¯æœç´¢å…³é”®è¯ ---
        for kw in keywords:
            print(f"\nğŸš€ Processing Keyword: {kw}")
            
            # ã€æ ¸å¿ƒä¿®æ”¹ã€‘æ¯æ¬¡æœæ–°è¯éƒ½é‡æ–°æ‰“å¼€é¦–é¡µï¼Œç¡®ä¿ç¯å¢ƒå¹²å‡€
            try:
                page.get('http://tonkiang.us/')
                handle_cloudflare(page) # æ¯æ¬¡éƒ½æ£€æŸ¥ä¸€ä¸‹ç›¾
                
                # å¯»æ‰¾è¾“å…¥æ¡†
                search_input = page.ele('tag:input@@type!=hidden', timeout=5)
                if search_input:
                    search_input.clear()
                    search_input.input(f"{kw}\n")
                    print(f"   - Searching for {kw}...")
                    page.wait(3) # ç­‰å¾…ç»“æœåŠ è½½
                else:
                    print(f"âŒ Input box not found for {kw}, skipping.")
                    continue

                # æå–é“¾æ¥
                items = page.eles('text:://')
                found_count = 0
                
                for item in items:
                    try:
                        # 1. æå– URL
                        txt = item.text
                        url_match = re.search(r'((?:http|https|rtmp|rtsp)://[^\s<>"\u4e00-\u9fa5]+)', txt)
                        if not url_match: continue
                        url = url_match.group(1)

                        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘å»é‡ï¼šå¦‚æœè¿™ä¸ªé“¾æ¥å·²ç»æŠ“è¿‡ï¼Œå°±è·³è¿‡
                        if url in seen_urls:
                            continue

                        # 2. æå–å¹¶æ£€æŸ¥æ—¥æœŸ
                        container = item
                        date_str = ""
                        # å‘ä¸Šæ‰¾ 3 å±‚çˆ¶çº§å…ƒç´ çœ‹çœ‹æœ‰æ²¡æœ‰æ—¥æœŸ
                        for _ in range(3):
                            container = container.parent()
                            if not container: break
                            mat = re.search(r'(\d{2,4}-\d{1,2}-\d{2,4})', container.text)
                            if mat:
                                date_str = mat.group(1)
                                break
                        
                        # 3. éªŒè¯æ—¥æœŸæœ‰æ•ˆæ€§
                        if date_str:
                            try:
                                if len(date_str.split('-')[0]) == 4:
                                    dt = datetime.strptime(date_str, '%Y-%m-%d')
                                else:
                                    dt = datetime.strptime(date_str, '%m-%d-%Y')
                                
                                if dt >= time_threshold:
                                    # åªæœ‰æ—¥æœŸç¬¦åˆæ‰åŠ å…¥
                                    final_results.append(f"{kw},{url}")
                                    seen_urls.add(url) # æ ‡è®°ä¸ºå·²æŠ“å–
                                    found_count += 1
                                    print(f"     -> Found: {date_str} | {url[:30]}...")
                            except: pass
                    except: continue
                
                print(f"   - {kw}: Added {found_count} new unique links.")

            except Exception as e:
                print(f"âŒ Error scraping {kw}: {e}")
                continue

    except Exception as e:
        print(f"âŒ Global Error: {e}")
    finally:
        if page: page.quit()
        try: shutil.rmtree(temp_user_dir)
        except: pass

# --- 4. æ™ºèƒ½ä¿å­˜ (ç†”æ–­æœºåˆ¶) ---
    print(f"\nğŸ“Š --- Result Analysis ---")
    new_count = len(final_results)
    print(f"   New results found: {new_count}")

    # 1. è·å–æ—§æ–‡ä»¶çš„é¢‘é“æ•°é‡
    old_count = 0
    if os.path.exists("tv.m3u"):
        with open("tv.m3u", "r", encoding="utf-8") as f:
            # ç»Ÿè®¡æœ‰å¤šå°‘è¡Œæ˜¯ #EXTINF å¼€å¤´çš„ï¼Œå°±æ˜¯å¤šå°‘ä¸ªé¢‘é“
            old_count = len([line for line in f if line.startswith("#EXTINF")])
    print(f"   Existing file count: {old_count}")

    # 2. è®¾å®šç†”æ–­é˜ˆå€¼ (ä¾‹å¦‚ï¼šä¸èƒ½ä½äºæ—§æ•°æ®çš„ 50%ï¼Œä¸”è‡³å°‘è¦æœ‰ 5 ä¸ª)
    # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œ(old_count=0)ï¼Œåˆ™æ²¡æœ‰é˜ˆå€¼é™åˆ¶
    threshold = int(old_count * 0.5)
    
    # 3. æ ¸å¿ƒåˆ¤æ–­é€»è¾‘
    save_changes = False
    
    if new_count == 0:
        print("âŒ No data found. Keeping previous file.")
    elif old_count > 0 and new_count < 5:
        # å¦‚æœä»¥å‰æœ‰æ•°æ®ï¼Œä½†ä»Šå¤©åªæŠ“åˆ°ä¸åˆ°5ä¸ªï¼Œåˆ¤å®šä¸ºå¼‚å¸¸
        print(f"âŒ Result too low (Only {new_count}). Possible failure. Keeping previous file ({old_count} items).")
    elif old_count > 0 and new_count < threshold:
        # å¦‚æœæš´è·Œè¶…è¿‡ 50%
        print(f"âš ï¸ Safety trigger! Count dropped from {old_count} to {new_count} (Limit: {threshold}). Keeping previous file.")
    else:
        # ä¸€åˆ‡æ­£å¸¸ (æˆ–è€…æ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œ)ï¼Œä¿å­˜ï¼
        save_changes = True
        print("âœ… Data looks good. Updating file...")

    # 4. æ‰§è¡Œä¿å­˜
    if save_changes:
        with open("tv.m3u", "w", encoding="utf-8") as f:
            f.write("#EXTM3U\n")
            for item in final_results:
                try:
                    name, url = item.split(',')
                    f.write(f"#EXTINF:-1,{name}\n{url}\n")
                except: pass

        with open("tv.txt", "w", encoding="utf-8") as f:
            f.write("\n".join(final_results))
            
        print(f"ğŸ’¾ Saved {new_count} items to tv.m3u and tv.txt")

if __name__ == "__main__":
    main()

from DrissionPage import ChromiumPage, ChromiumOptions
from datetime import datetime, timedelta
import re
import os
import time

def main():
    # --- GitHub Actions ä¸“ç”¨é…ç½® ---
    co = ChromiumOptions()
    co.set_argument('--headless=new')
    co.set_argument('--no-sandbox')
    co.set_argument('--disable-gpu')
    co.set_argument('--disable-dev-shm-usage')
    
    # ğŸ‘‡ã€ä¸‰é‡ä¿®å¤ã€‘åŒæ—¶åŠ ä¸Šè¿™ä¸‰ä¸ªå‚æ•°ï¼Œç¡®ä¿ä¸‡æ— ä¸€å¤±
    co.set_argument('--remote-debugging-port=9222')
    co.set_argument('--remote-allow-origins=*')
    co.set_argument('--bind-address=0.0.0.0') 
    
    # è‡ªåŠ¨è¯»å– GitHub Actions è®¾ç½®çš„æµè§ˆå™¨è·¯å¾„
    chrome_path = os.getenv('CHROME_PATH')
    if chrome_path:
        print(f"ğŸ”§ Using Chrome at: {chrome_path}")
        co.set_paths(browser_path=chrome_path)

    # ã€è°ƒè¯•ã€‘æ‰“å°æœ€ç»ˆå‚æ•°ï¼Œç¡®è®¤ä¿®å¤æ˜¯å¦ç”Ÿæ•ˆ
    print(f"ğŸ”§ Browser Args: {co.arguments}")

    try:
        page = ChromiumPage(co)
        print("âœ… Browser launched successfully!")
    except Exception as e:
        print(f"âŒ Browser Init Failed: {e}")
        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•ä¸æŒ‡å®šç«¯å£è®©å®ƒè‡ªå·±éšæœºï¼ˆæœ€åçš„æŒ£æ‰ï¼‰
        return

    # --- é‡‡é›†é€»è¾‘ ---
    keywords = ["æ— çº¿æ–°é—»", "å¹¿ä¸œä½“è‚²", "ç¿¡ç¿ å°"]
    days_limit = 60
    final_results = []
    time_threshold = datetime.now() - timedelta(days=days_limit)

    try:
        print(f"ğŸš€ Start scraping...")
        page.get('http://tonkiang.us/')
        time.sleep(2)
        print(f"ğŸ“„ Page Title: {page.title}")

        for kw in keywords:
            print(f"ğŸ” Checking: {kw}...")
            try:
                # å¯»æ‰¾è¾“å…¥æ¡†
                search_input = page.ele('tag:input@@type!=hidden', timeout=5)
                if search_input:
                    search_input.clear()
                    search_input.input(f"{kw}\n")
                    page.wait(3)
                else:
                    print("âŒ Input not found, refreshing...")
                    page.refresh()
                    continue
            except: continue

            # é‡‡é›†é“¾æ¥
            items = page.eles('text:://')
            print(f"   - Found {len(items)} links on page")
            
            for item in items:
                try:
                    txt = item.text
                    url_match = re.search(r'((?:http|https|rtmp|rtsp)://[^\s<>"\u4e00-\u9fa5]+)', txt)
                    if not url_match: continue
                    url = url_match.group(1)

                    container = item
                    for _ in range(3):
                        container = container.parent()
                        if not container: break
                        mat = re.search(r'(\d{2,4}-\d{1,2}-\d{2,4})', container.text)
                        if mat:
                            date_str = mat.group(1)
                            # æ—¥æœŸè§£æ
                            try:
                                if len(date_str.split('-')[0]) == 4:
                                    dt = datetime.strptime(date_str, '%Y-%m-%d')
                                else:
                                    dt = datetime.strptime(date_str, '%m-%d-%Y')
                                
                                if dt >= time_threshold:
                                    final_results.append(f"{kw},{url}")
                                    print(f"     -> Valid: {date_str}")
                                    break
                            except: pass
                except: continue

    except Exception as e:
        print(f"âŒ Global Error: {e}")
    finally:
        page.quit()

    # --- å¼ºåˆ¶ä¿å­˜æ–‡ä»¶ ---
    print(f"ğŸ’¾ Saving {len(final_results)} items...")
    
    unique_data = list(dict.fromkeys(final_results))
    
    with open("tv.m3u", "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        if not unique_data:
            f.write("# No data found in this run.\n")
        for item in unique_data:
            try:
                name, url = item.split(',')
                f.write(f"#EXTINF:-1,{name}\n{url}\n")
            except: pass

    with open("tv.txt", "w", encoding="utf-8") as f:
        if not unique_data:
            f.write("No data found.")
        else:
            f.write("\n".join(unique_data))

if __name__ == "__main__":
    main()

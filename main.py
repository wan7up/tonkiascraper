from DrissionPage import ChromiumPage, ChromiumOptions
from datetime import datetime, timedelta
import re
import os
import time
import tempfile
import shutil
import csv

# --- é…ç½®éƒ¨åˆ† ---
KEYWORDS = ["æ— çº¿æ–°é—»", "å¹¿ä¸œä½“è‚²", "ç¿¡ç¿ å°"] # åœ¨è¿™é‡Œä¿®æ”¹ä½ çš„æœç´¢è¯
DAYS_LIMIT = 30  # æœ‰æ•ˆæœŸ 30 å¤©
DATA_FILE = "data.csv" # æ ¸å¿ƒæ•°æ®åº“æ–‡ä»¶
M3U_FILE = "tv.m3u"
TXT_FILE = "tv.txt"

def handle_cloudflare(page):
    """æ™ºèƒ½å¤„ç† Cloudflare"""
    print("ğŸ›¡ï¸ Checking Cloudflare status...")
    for i in range(10):
        try:
            title = page.title
            if "Just a moment" not in title and ("IPTV" in title or "Search" in title or "Tonkiang" in title):
                print(f"âœ… Access Granted! (Title: {title})")
                return True
            print(f"   - Still in waiting room... ({i+1}/10)")
            time.sleep(3)
        except:
            time.sleep(3)
    print("âš ï¸ Cloudflare check timed out")
    return False

def clean_channel_name(text):
    """æ¸…ç†å°åï¼Œå»é™¤å¤šä½™ç©ºæ ¼å’Œæ— å…³å­—ç¬¦"""
    # æå–ä¸»è¦æ–‡å­—ï¼Œå»æ‰å¯èƒ½çš„ CSS å¹²æ‰°
    text = text.replace('\n', ' ').strip()
    return text

def load_history():
    """è¯»å–å†å²æ•°æ® (URL -> {name, date, keyword})"""
    history = {}
    if os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, 'r', encoding='utf-8', newline='') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    # key æ˜¯ URLï¼Œvalue æ˜¯å…¶ä»–ä¿¡æ¯
                    history[row['URL']] = {
                        'Channel': row['Channel'],
                        'Date': row['Date'],
                        'Keyword': row['Keyword']
                    }
            print(f"ğŸ“– Loaded {len(history)} items from history.")
        except Exception as e:
            print(f"âš ï¸ Error loading history: {e}")
    return history

def save_data(data_dict):
    """ä¿å­˜æ•°æ®åˆ° CSV å’Œ M3U"""
    # 1. ä¿å­˜ CSV (ä½œä¸ºä¸‹æ¬¡çš„å†å²æ•°æ®)
    try:
        with open(DATA_FILE, 'w', encoding='utf-8', newline='') as f:
            fieldnames = ['Keyword', 'Channel', 'Date', 'URL']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            # æŒ‰å…³é”®å­—æ’åºï¼Œå¥½çœ‹ä¸€ç‚¹
            sorted_items = sorted(data_dict.items(), key=lambda x: x[1]['Keyword'])
            
            for url, info in sorted_items:
                writer.writerow({
                    'Keyword': info['Keyword'],
                    'Channel': info['Channel'],
                    'Date': info['Date'],
                    'URL': url
                })
        print(f"ğŸ’¾ Updated {DATA_FILE} with {len(data_dict)} items.")
    except Exception as e:
        print(f"âŒ Error saving CSV: {e}")

    # 2. ç”Ÿæˆ M3U (ä¾›å¤–éƒ¨ä½¿ç”¨)
    try:
        with open(M3U_FILE, 'w', encoding='utf-8') as f:
            f.write("#EXTM3U\n")
            for url, info in data_dict.items():
                # æ ¼å¼: #EXTINF:-1 group-title="å…³é”®å­—",å°å
                f.write(f'#EXTINF:-1 group-title="{info["Keyword"]}",{info["Channel"]}\n{url}\n')
        print(f"ğŸ“º Generated {M3U_FILE}")
    except Exception as e:
        print(f"âŒ Error saving M3U: {e}")

    # 3. ç”Ÿæˆ TXT
    try:
        with open(TXT_FILE, 'w', encoding='utf-8') as f:
            for url, info in data_dict.items():
                f.write(f'{info["Channel"]},{url}\n')
        print(f"ğŸ“ Generated {TXT_FILE}")
    except Exception as e:
        print(f"âŒ Error saving TXT: {e}")

def main():
    # --- 1. ç¯å¢ƒé…ç½® ---
    temp_user_dir = tempfile.mkdtemp()
    co = ChromiumOptions()
    co.headless(True)
    co.set_argument('--no-sandbox')
    co.set_argument('--disable-gpu')
    co.set_argument('--disable-dev-shm-usage')
    co.set_argument(f'--user-data-dir={temp_user_dir}')
    co.set_argument('--remote-allow-origins=*')
    co.set_user_agent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36')

    chrome_path = os.getenv('CHROME_PATH')
    if chrome_path:
        co.set_paths(browser_path=chrome_path)

    try:
        page = ChromiumPage(co)
        print("âœ… Browser launched successfully!")
    except Exception as e:
        print(f"âŒ Browser Init Failed: {e}")
        try: shutil.rmtree(temp_user_dir) 
        except: pass
        return

    # --- 2. åŠ è½½å†å²æ•°æ® ---
    all_data = load_history() # æ ¼å¼: {url: {Channel, Date, Keyword}}
    current_date = datetime.now()
    cutoff_date = current_date - timedelta(days=DAYS_LIMIT)

    try:
        # --- 3. å¼€å§‹æŠ“å–æ–°æ•°æ® ---
        for kw in KEYWORDS:
            print(f"\nğŸš€ Processing Keyword: {kw}")
            try:
                page.get('http://tonkiang.us/')
                handle_cloudflare(page)
                
                search_input = page.ele('tag:input@@type!=hidden', timeout=5)
                if search_input:
                    search_input.clear()
                    search_input.input(f"{kw}\n")
                    print(f"   - Searching for {kw}...")
                    
                    # æ™ºèƒ½ç­‰å¾…
                    for i in range(10):
                        items = page.eles('text:://')
                        if len(items) > 5:
                            print("     -> Results loaded!")
                            break
                        time.sleep(1.5)
                else:
                    print(f"âŒ Input not found for {kw}")
                    continue

                # æå–æ•°æ®
                new_count = 0
                for item in items:
                    try:
                        # æå–é“¾æ¥
                        txt = item.text
                        url_match = re.search(r'((?:http|https|rtmp|rtsp)://[^\s<>"\u4e00-\u9fa5]+)', txt)
                        if not url_match: continue
                        url = url_match.group(1)

                        # æå–æ—¥æœŸ
                        container = item
                        date_str = ""
                        channel_name = kw # é»˜è®¤å°åä¸ºå…³é”®å­—ï¼Œä¸‹é¢å°è¯•ä»é¡µé¢æå–
                        
                        # å‘ä¸Šæ‰¾çˆ¶çº§è·å–æ—¥æœŸï¼ŒåŒæ—¶æ‰¾å°å
                        # Tonkiang ç»“æ„é€šå¸¸æ˜¯: <div> ç»“æœæ–‡å­—... æ—¥æœŸ... <a href=...>Link</a> </div>
                        # æˆ‘ä»¬å°è¯•è·å–æ•´è¡Œçš„æ–‡æœ¬ä½œä¸ºå°åæ¥æº
                        for i in range(3):
                            container = container.parent()
                            if not container: break
                            
                            # æ‰¾æ—¥æœŸ
                            if not date_str:
                                mat = re.search(r'(\d{2,4}-\d{1,2}-\d{2,4})', container.text)
                                if mat: date_str = mat.group(1)
                            
                            # æ‰¾å°å (ç®€å•å¤„ç†ï¼šå–çˆ¶çº§æ–‡æœ¬ï¼Œå»æ‰é“¾æ¥å’Œæ—¥æœŸï¼Œå‰©ä¸‹çš„å°±æ˜¯å¯èƒ½çš„å°å)
                            # è¿™æ˜¯ä¸€ä¸ªç²—ç•¥çš„æå–ï¼Œå› ä¸ºç½‘é¡µç»“æ„å¤šå˜
                            full_text = container.text
                            if kw in full_text: # ç¡®ä¿è¿™è¡Œæ–‡å­—é‡ŒåŒ…å«äº†å…³é”®å­—ï¼Œæ‰è®¤ä¸ºæ˜¯å°å
                                # ç®€å•çš„æ¸…æ´—é€»è¾‘
                                temp_name = full_text.split('http')[0].split(date_str)[0].strip()
                                if len(temp_name) > 0 and len(temp_name) < 50:
                                    channel_name = clean_channel_name(temp_name)

                        # æ ¼å¼åŒ–æ—¥æœŸ
                        final_date = None
                        if date_str:
                            try:
                                if len(date_str.split('-')[0]) == 4:
                                    dt = datetime.strptime(date_str, '%Y-%m-%d')
                                else:
                                    dt = datetime.strptime(date_str, '%m-%d-%Y')
                                final_date = dt
                            except: pass
                        
                        # é€»è¾‘åˆ¤æ–­ï¼šæ–°å¢æˆ–æ›´æ–°
                        if final_date:
                            str_date = final_date.strftime('%Y-%m-%d')
                            
                            # å¦‚æœé“¾æ¥å·²å­˜åœ¨
                            if url in all_data:
                                # æ›´æ–°æ—¥æœŸä¸ºæœ€æ–°çš„
                                old_date_str = all_data[url]['Date']
                                try:
                                    old_date = datetime.strptime(old_date_str, '%Y-%m-%d')
                                    if final_date > old_date:
                                        all_data[url]['Date'] = str_date
                                        # å¯ä»¥é€‰æ‹©æ›´æ–°å°åï¼Œä¹Ÿå¯ä»¥ä¿ç•™æ—§çš„ï¼Œè¿™é‡Œé€‰æ‹©ä¿ç•™æ—§çš„å°åé™¤éæ—§çš„ä¸ºç©º
                                        if not all_data[url]['Channel']:
                                            all_data[url]['Channel'] = channel_name
                                        # print(f"     -> Updated: {channel_name} ({str_date})")
                                except: pass
                            else:
                                # æ–°å¢é“¾æ¥
                                all_data[url] = {
                                    'Keyword': kw,
                                    'Channel': channel_name,
                                    'Date': str_date
                                }
                                new_count += 1
                                print(f"     -> New: {channel_name} | {str_date}")

                    except Exception as e: continue
                
                print(f"   - Added {new_count} new links for {kw}")

            except Exception as e:
                print(f"âŒ Error processing {kw}: {e}")

    except Exception as e:
        print(f"âŒ Global Error: {e}")
    finally:
        if page: page.quit()
        try: shutil.rmtree(temp_user_dir)
        except: pass

    # --- 4. æ¸…ç†è¿‡æœŸæ•°æ® & ä¿å­˜ ---
    print("\nğŸ§¹ Cleaning old data...")
    valid_data = {}
    expired_count = 0
    
    for url, info in all_data.items():
        try:
            item_date = datetime.strptime(info['Date'], '%Y-%m-%d')
            # æ ¸å¿ƒä¿ç•™é€»è¾‘ï¼šåªæœ‰æ—¥æœŸåœ¨ 30 å¤©ä»¥å†…çš„ä¿ç•™
            if item_date >= cutoff_date:
                valid_data[url] = info
            else:
                expired_count += 1
        except:
            # æ—¥æœŸæ ¼å¼é”™è¯¯çš„ä¹Ÿåˆ æ‰
            expired_count += 1

    print(f"   Removed {expired_count} expired items (older than {cutoff_date.strftime('%Y-%m-%d')})")
    print(f"   Total valid items: {len(valid_data)}")

    # 5. å®‰å…¨ä¿å­˜ (åªæœ‰å½“æœ‰æ•ˆæ•°æ®å¤§äº0æ—¶æ‰ä¿å­˜ï¼Œé˜²æ­¢å…¨åˆ å…‰äº†)
    if len(valid_data) > 0:
        save_data(valid_data)
    else:
        print("âš ï¸ No valid data remaining! Skipping save to protect old files.")

if __name__ == "__main__":
    main()

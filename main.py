from DrissionPage import ChromiumPage, ChromiumOptions
from datetime import datetime, timedelta
import re
import os
import time
import tempfile
import shutil
import csv

# --- é…ç½®éƒ¨åˆ† ---
KEYWORDS = ["æ— çº¿æ–°é—»", "å¹¿ä¸œä½“è‚²", "ç¿¡ç¿ å°", "VIU", "tvb plus", "Now Sports ç²¾é¸", "Discovery", "åœ‹å®¶åœ°ç†", "NatGeo", "HBO"]
DAYS_LIMIT = 30  
DATA_FILE = "data.csv" 
M3U_FILE = "tv.m3u"
TXT_FILE = "tv.txt"

def handle_cloudflare(page):
    """æ™ºèƒ½å¤„ç† Cloudflare"""
    for i in range(10):
        try:
            title = page.title
            if "Just a moment" not in title and ("IPTV" in title or "Search" in title or "Tonkiang" in title):
                return True
            time.sleep(2)
        except:
            time.sleep(2)
    return False

def clean_channel_name(text):
    return text.replace('\n', ' ').strip()

def load_history():
    """è¯»å–å†å²æ•°æ®"""
    history = {}
    if os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, 'r', encoding='utf-8', newline='') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    history[row['URL']] = {
                        'Channel': row['Channel'],
                        'Date': row['Date'],
                        'Keyword': row['Keyword']
                    }
            print(f"ğŸ“– Loaded {len(history)} items from history.")
        except Exception as e:
            print(f"âš ï¸ Error loading history: {e}")
    return history

def save_data(data_dict):
    """ä¿å­˜æ•°æ®"""
    try:
        # 1. CSV
        with open(DATA_FILE, 'w', encoding='utf-8', newline='') as f:
            fieldnames = ['Keyword', 'Channel', 'Date', 'URL']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            sorted_items = sorted(data_dict.items(), key=lambda x: x[1]['Keyword'])
            for url, info in sorted_items:
                writer.writerow({'Keyword': info['Keyword'], 'Channel': info['Channel'], 'Date': info['Date'], 'URL': url})
        
        # 2. M3U
        with open(M3U_FILE, 'w', encoding='utf-8') as f:
            f.write("#EXTM3U\n")
            for url, info in data_dict.items():
                f.write(f'#EXTINF:-1 group-title="{info["Keyword"]}",{info["Channel"]}\n{url}\n')
        
        # 3. TXT
        with open(TXT_FILE, 'w', encoding='utf-8') as f:
            for url, info in data_dict.items():
                f.write(f'{info["Channel"]},{url}\n')
                
        print(f"ğŸ’¾ All files updated. Total unique items: {len(data_dict)}")
    except Exception as e:
        print(f"âŒ Error saving files: {e}")

def main():
    # --- 1. ç¯å¢ƒé…ç½® ---
    temp_user_dir = tempfile.mkdtemp()
    co = ChromiumOptions()
    co.headless(True)
    co.set_argument('--no-sandbox')
    co.set_argument('--disable-gpu')
    co.set_argument('--disable-dev-shm-usage')
    co.set_argument(f'--user-data-dir={temp_user_dir}')
    co.set_argument('--remote-allow-origins=*')
    co.set_user_agent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36')

    chrome_path = os.getenv('CHROME_PATH')
    if chrome_path:
        co.set_paths(browser_path=chrome_path)

    try:
        page = ChromiumPage(co)
        print("âœ… Browser launched successfully!")
    except Exception as e:
        print(f"âŒ Browser Init Failed: {e}")
        try: shutil.rmtree(temp_user_dir) 
        except: pass
        return

    # --- 2. åŠ è½½å†å²æ•°æ® ---
    all_data = load_history()
    current_date = datetime.now()
    cutoff_date = current_date - timedelta(days=DAYS_LIMIT)

    try:
        # --- 3. å¾ªç¯æœç´¢å…³é”®è¯ ---
        for kw in KEYWORDS:
            print(f"\nğŸš€ Processing Keyword: {kw}")
            
            try:
                # 1. æ‰“å¼€é¦–é¡µ
                page.get('http://tonkiang.us/')
                if not handle_cloudflare(page):
                    print("   - Cloudflare check failed, skipping...")
                    continue
                
                # 2. å¯»æ‰¾è¾“å…¥æ¡†
                search_input = page.ele('tag:input@@type!=hidden', timeout=5)
                if not search_input:
                    print("âŒ Input not found")
                    continue
                
                # 3. è¾“å…¥å…³é”®å­—
                search_input.clear()
                search_input.input(kw)
                
                # 4. ã€æ ¸å¿ƒä¿®å¤ã€‘ä½¿ç”¨ JS æš´åŠ›æäº¤è¡¨å•
                # ä¸å†å¯»æ‰¾æŒ‰é’®ï¼Œè€Œæ˜¯ç›´æ¥æ‰¾åˆ°è¾“å…¥æ¡†æ‰€å±çš„ Formï¼Œå¼ºåˆ¶ Submit
                print("   - Submitting form via JS...")
                try:
                    # å°è¯•æ‰¾åˆ°è¾“å…¥æ¡†çš„çˆ¶çº§ Form å…ƒç´ å¹¶æäº¤
                    # è¿™è¡Œä»£ç çš„æ„æ€æ˜¯ï¼šæ‰¾åˆ° search_input çš„çˆ¶çº§ form æ ‡ç­¾ï¼Œç„¶åæ‰§è¡Œ submit()
                    form = search_input.parent('tag:form')
                    if form:
                        # ä½¿ç”¨ DrissionPage çš„ run_js ç›´æ¥æ‰§è¡ŒåŸç”Ÿ JS æäº¤ï¼Œæœ€ç¨³
                        page.run_js('arguments[0].submit()', form)
                    else:
                        # å¦‚æœæ‰¾ä¸åˆ° form æ ‡ç­¾ï¼Œå°è¯•å›è½¦å…œåº•
                        search_input.input('\n')
                except Exception as e:
                    print(f"   âš ï¸ JS Submit failed: {e}, trying Enter key.")
                    search_input.input('\n')

                # 5. ç­‰å¾…åŠ è½½ (ç§»é™¤æ•°é‡æ£€æŸ¥ï¼Œæ”¹ä¸ºçº¯ç­‰å¾…)
                page.wait.load_start()
                
                # ç®€å•çš„åŠ¨æ€ç­‰å¾…ï¼šåªè¦æœ‰ç»“æœå°±è¡Œï¼Œä¸åˆ¤æ–­æ•°é‡æ˜¯å¦è¾¾æ ‡
                found_items = []
                prev_count = -1
                
                # æœ€å¤šç­‰ 8 ç§’
                for i in range(8):
                    found_items = page.eles('text:://') # å¯»æ‰¾æ‰€æœ‰å¸¦ :// çš„æ–‡æœ¬
                    count = len(found_items)
                    
                    # åªè¦æ•°é‡ç¨³å®šäº†ï¼ˆä¸å†å˜åŒ–ï¼‰ï¼Œå°±è®¤ä¸ºåŠ è½½å®Œäº†
                    if count > 0 and count == prev_count:
                         break
                    
                    prev_count = count
                    time.sleep(1)

                print(f"     -> Found {len(found_items)} potential links. Processing...")

                # 6. æå–æ•°æ® (é æ­£åˆ™å’Œæ—¥æœŸè¿‡æ»¤åƒåœ¾)
                new_count = 0
                for item in found_items:
                    try:
                        # æå–é“¾æ¥
                        txt = item.text
                        url_match = re.search(r'((?:http|https|rtmp|rtsp)://[^\s<>"\u4e00-\u9fa5]+)', txt)
                        if not url_match: continue
                        url = url_match.group(1)

                        # æå–æ—¥æœŸ (è¿™æ˜¯åŒºåˆ†â€œçœŸç»“æœâ€å’Œâ€œé¦–é¡µå¹¿å‘Šâ€çš„å…³é”®)
                        container = item
                        date_str = ""
                        channel_name = kw 
                        
                        for i in range(3):
                            container = container.parent()
                            if not container: break
                            
                            if not date_str:
                                mat = re.search(r'(\d{2,4}-\d{1,2}-\d{2,4})', container.text)
                                if mat: date_str = mat.group(1)
                            
                            # æå–å°å
                            full_text = container.text
                            if kw in full_text:
                                temp_name = full_text.split('http')[0].split(date_str)[0].strip()
                                if len(temp_name) > 0 and len(temp_name) < 50:
                                    channel_name = clean_channel_name(temp_name)

                        # ã€æ ¸å¿ƒè¿‡æ»¤ã€‘åªæœ‰æ‰¾åˆ°äº†æœ‰æ•ˆæ—¥æœŸï¼Œæ‰è®¤ä¸ºæ˜¯æœ‰æ•ˆç»“æœ
                        # é¦–é¡µçš„â€œè”ç³»æˆ‘ä»¬â€é“¾æ¥å‘¨å›´æ˜¯ä¸ä¼šæœ‰æ—¥æœŸçš„ï¼Œä¼šè¢«è¿™é‡Œè‡ªåŠ¨è¿‡æ»¤
                        final_date = None
                        if date_str:
                            try:
                                if len(date_str.split('-')[0]) == 4:
                                    dt = datetime.strptime(date_str, '%Y-%m-%d')
                                else:
                                    dt = datetime.strptime(date_str, '%m-%d-%Y')
                                final_date = dt
                            except: pass
                        
                        if final_date:
                            str_date = final_date.strftime('%Y-%m-%d')
                            
                            # åˆå¹¶/æ›´æ–°é€»è¾‘
                            if url in all_data:
                                old_date = datetime.strptime(all_data[url]['Date'], '%Y-%m-%d')
                                if final_date > old_date:
                                    all_data[url]['Date'] = str_date
                                    # å¦‚æœæ—§åå­—æ˜¯é»˜è®¤å…³é”®å­—ï¼Œæ–°åå­—æ›´è¯¦ç»†ï¼Œåˆ™æ›´æ–°åå­—
                                    if all_data[url]['Channel'] == kw and channel_name != kw:
                                        all_data[url]['Channel'] = channel_name
                            else:
                                all_data[url] = {'Keyword': kw, 'Channel': channel_name, 'Date': str_date}
                                new_count += 1
                    except: continue
                
                print(f"   -> Validated & Added: {new_count} new unique links.")

            except Exception as e:
                print(f"âŒ Error processing {kw}: {e}")

    except Exception as e:
        print(f"âŒ Global Error: {e}")
    finally:
        if page: page.quit()
        try: shutil.rmtree(temp_user_dir)
        except: pass

    # --- 4. æ¸…ç†ä¸ä¿å­˜ ---
    print("\nğŸ§¹ Cleaning old data...")
    valid_data = {}
    expired_count = 0
    
    for url, info in all_data.items():
        try:
            item_date = datetime.strptime(info['Date'], '%Y-%m-%d')
            if item_date >= cutoff_date:
                valid_data[url] = info
            else:
                expired_count += 1
        except:
            expired_count += 1

    print(f"   Removed {expired_count} expired items.")

    if len(valid_data) > 0:
        save_data(valid_data)
    else:
        print("âš ï¸ No valid data remaining! Skipping save.")

if __name__ == "__main__":
    main()

from DrissionPage import ChromiumPage, ChromiumOptions
from datetime import datetime, timedelta
import re
import os
import time
import tempfile
import shutil
import csv # å¼•å…¥ CSV

def handle_cloudflare(page):
    """(ä¿æŒåŸç‰ˆ) æ™ºèƒ½å¤„ç† Cloudflare"""
    print("ğŸ›¡ï¸ Checking Cloudflare status...")
    for i in range(10):
        try:
            title = page.title
            if "Just a moment" not in title and ("IPTV" in title or "Search" in title or "Tonkiang" in title):
                print(f"âœ… Access Granted! (Title: {title})")
                return True
            time.sleep(3)
        except:
            time.sleep(3)
    print("âš ï¸ Cloudflare check timed out")
    return False

# --- æ–°å¢ï¼šè¯»å–å†å² CSV ---
def load_history():
    history = {}
    if os.path.exists("data.csv"):
        try:
            with open("data.csv", 'r', encoding='utf-8', newline='') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    history[row['URL']] = {
                        'Channel': row['Channel'],
                        'Date': row['Date'],
                        'Keyword': row['Keyword']
                    }
            print(f"ğŸ“– Loaded {len(history)} items from history.")
        except: pass
    return history

# --- æ–°å¢ï¼šä¿å­˜æ‰€æœ‰æ–‡ä»¶ ---
def save_files(data_dict):
    try:
        # 1. ä¿å­˜ CSV
        with open("data.csv", 'w', encoding='utf-8', newline='') as f:
            fieldnames = ['Keyword', 'Channel', 'Date', 'URL']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            sorted_items = sorted(data_dict.items(), key=lambda x: x[1]['Keyword'])
            for url, info in sorted_items:
                writer.writerow({'Keyword': info['Keyword'], 'Channel': info['Channel'], 'Date': info['Date'], 'URL': url})
        
        # 2. ä¿å­˜ M3U
        with open("tv.m3u", 'w', encoding='utf-8') as f:
            f.write("#EXTM3U\n")
            for url, info in data_dict.items():
                f.write(f'#EXTINF:-1 group-title="{info["Keyword"]}",{info["Channel"]}\n{url}\n')

        # 3. ä¿å­˜ TXT
        with open("tv.txt", 'w', encoding='utf-8') as f:
            for url, info in data_dict.items():
                f.write(f'{info["Channel"]},{url}\n')

        print(f"ğŸ’¾ Database updated: {len(data_dict)} items saved.")
    except Exception as e:
        print(f"âŒ Save failed: {e}")

def main():
    # --- 1. ç¯å¢ƒé…ç½® (ä¿æŒåŸç‰ˆ) ---
    temp_user_dir = tempfile.mkdtemp()
    co = ChromiumOptions()
    co.headless(True)
    co.set_argument('--no-sandbox')
    co.set_argument('--disable-gpu')
    co.set_argument('--disable-dev-shm-usage')
    # è¿™é‡Œçš„çª—å£å¤§å°å¾ˆé‡è¦ï¼Œé˜²æ­¢æŒ‰é’®è¢«æŒ¡ä½
    co.set_argument('--window-size=1920,1080')
    co.set_argument(f'--user-data-dir={temp_user_dir}')
    co.set_argument('--remote-allow-origins=*')
    co.set_user_agent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36')

    chrome_path = os.getenv('CHROME_PATH')
    if chrome_path:
        co.set_paths(browser_path=chrome_path)

    try:
        page = ChromiumPage(co)
        print("âœ… Browser launched successfully!")
    except Exception as e:
        print(f"âŒ Browser Init Failed: {e}")
        try: shutil.rmtree(temp_user_dir) 
        except: pass
        return

    # --- 2. å‡†å¤‡æ•°æ® ---
    all_data = load_history()
    
    # ä½ çš„åŸç‰ˆå…³é”®è¯
    keywords = ["æ— çº¿æ–°é—»", "å¹¿ä¸œä½“è‚²", "ç¿¡ç¿ å°", "VIU", "tvb plus", "NatGeo_twn", "Now Sports ç²¾é¸", "discoveryhd_twn", "tlc_twn", "åœ‹å®¶åœ°ç†", "hbohd_twn"]
    days_limit = 30
    current_date = datetime.now()
    cutoff_date = current_date - timedelta(days=days_limit)

    try:
        # --- 3. å¾ªç¯æœç´¢ ---
        for kw in keywords:
            print(f"\nğŸš€ Processing Keyword: {kw}")
            
            try:
                page.get('http://tonkiang.us/')
                handle_cloudflare(page) 
                
                search_input = page.ele('tag:input@@type!=hidden', timeout=5)
                if search_input:
                    search_input.clear()
                    # åŸç‰ˆç”¨çš„æ˜¯ f"{kw}\n"ï¼Œä½†è°ƒè¯•è¯æ˜å›è½¦å¤±æ•ˆäº†
                    # è¿™é‡Œæ”¹ä¸ºåªè¾“å­—ï¼Œåé¢æ‰‹åŠ¨ç‚¹æŒ‰é’®
                    search_input.input(kw)
                    
                    # ğŸ‘‡ğŸ‘‡ğŸ‘‡ å…³é”®ä¿®å¤ï¼šå¿…é¡»ç‰©ç†ç‚¹å‡»æŒ‰é’®æ‰èƒ½è·³å‡ºé¦–é¡µ ğŸ‘‡ğŸ‘‡ğŸ‘‡
                    try:
                        # å°è¯•æ‰¾è¾“å…¥æ¡†æ—è¾¹çš„æŒ‰é’®ï¼Œæˆ–è€… type=submit çš„æŒ‰é’®
                        search_btn = search_input.next('tag:button') or page.ele('tag:button@@type=submit')
                        if search_btn:
                            search_btn.click()
                        else:
                            # å®åœ¨æ‰¾ä¸åˆ°æ‰ç”¨å›è½¦å…œåº•
                            search_input.input('\n')
                    except:
                        search_input.input('\n')
                    
                    # ä¿æŒåŸç‰ˆçš„ç­‰å¾…æ—¶é—´
                    page.wait(3) 

                else:
                    print(f"âŒ Input box not found for {kw}, skipping.")
                    continue

                # --- æå–é€»è¾‘ (åŸºäºåŸç‰ˆï¼Œä½†ä¿®å¤å°åæå–) ---
                items = page.eles('text:://')
                new_found = 0
                
                # ç®€å•çš„æ£€æŸ¥ï¼šå¦‚æœè¿˜åœ¨é¦–é¡µï¼Œé€šå¸¸ text::// æ•°é‡å¾ˆå°‘æˆ–è€…å…¨æ˜¯ä¹±ä¸ƒå…«ç³Ÿçš„
                if len(items) > 0:
                    for item in items:
                        try:
                            # 1. æå–é“¾æ¥
                            txt = item.text
                            url_match = re.search(r'((?:http|https|rtmp|rtsp)://[^\s<>"\u4e00-\u9fa5]+)', txt)
                            if not url_match: continue
                            url = url_match.group(1)

                            # 2. æå–æ—¥æœŸå’Œå°å (å‘ä¸Šæ‰¾çˆ¶çº§)
                            container = item
                            date_str = ""
                            channel_name = kw # é»˜è®¤å€¼ï¼Œä¸‹é¢å°è¯•è¦†ç›–
                            
                            for _ in range(3):
                                container = container.parent()
                                if not container: break
                                
                                # æ‰¾æ—¥æœŸ
                                if not date_str:
                                    mat = re.search(r'(\d{2,4}-\d{1,2}-\d{2,4})', container.text)
                                    if mat: date_str = mat.group(1)
                                
                                # ğŸ‘‡ğŸ‘‡ğŸ‘‡ æ ¸å¿ƒä¿®æ”¹ï¼šæå–çœŸå®å°å ğŸ‘‡ğŸ‘‡ğŸ‘‡
                                # åªè¦è¿™è¡Œå­—é‡Œæœ‰å†…å®¹ï¼Œå°±å°è¯•åˆ‡å‰²å‡ºåå­—
                                full_text = container.text
                                # é€»è¾‘ï¼šç æ‰ http åé¢çš„ï¼Œå†ç æ‰æ—¥æœŸï¼Œå‰©ä¸‹çš„å°±æ˜¯åå­—
                                temp_text = full_text.split('http')[0]
                                if date_str:
                                    temp_text = temp_text.replace(date_str, '')
                                
                                clean_name = temp_text.strip().replace('\n', ' ')
                                # å¦‚æœå‰©ä¸‹çš„åå­—é•¿åº¦åˆç†(å¤§äº1ä¸”å°äº50)ï¼Œå°±é‡‡ç”¨å®ƒ
                                if len(clean_name) > 1 and len(clean_name) < 50:
                                    channel_name = clean_name

                            # 3. å­˜å…¥æ•°æ® (ç»“åˆ CSV é€»è¾‘)
                            if date_str:
                                try:
                                    if len(date_str.split('-')[0]) == 4:
                                        dt = datetime.strptime(date_str, '%Y-%m-%d')
                                    else:
                                        dt = datetime.strptime(date_str, '%m-%d-%Y')
                                    str_date = dt.strftime('%Y-%m-%d')

                                    # æ•°æ®åˆå¹¶ä¸æ›´æ–°
                                    if url in all_data:
                                        old_date = datetime.strptime(all_data[url]['Date'], '%Y-%m-%d')
                                        if dt > old_date:
                                            all_data[url]['Date'] = str_date
                                            # æ€»æ˜¯æ›´æ–°ä¸ºæœ€æ–°æŠ“åˆ°çš„åå­—
                                            all_data[url]['Channel'] = channel_name
                                    else:
                                        all_data[url] = {
                                            'Keyword': kw,
                                            'Channel': channel_name,
                                            'Date': str_date
                                        }
                                        new_found += 1
                                except: pass
                        except: continue
                
                print(f"   - {kw}: Processed. Found {new_found} new items.")

            except Exception as e:
                print(f"âŒ Error scraping {kw}: {e}")
                continue

    except Exception as e:
        print(f"âŒ Global Error: {e}")
    finally:
        if page: page.quit()
        try: shutil.rmtree(temp_user_dir)
        except: pass

    # --- 4. æ¸…ç†ä¸ä¿å­˜ (CSV é€»è¾‘) ---
    print("\nğŸ§¹ Cleaning old data...")
    valid_data = {}
    expired_count = 0
    
    for url, info in all_data.items():
        try:
            item_date = datetime.strptime(info['Date'], '%Y-%m-%d')
            if item_date >= cutoff_date:
                valid_data[url] = info
            else:
                expired_count += 1
        except:
            expired_count += 1

    print(f"   Removed {expired_count} expired items.")
    
    if len(valid_data) > 0:
        save_files(valid_data)
    else:
        print("âš ï¸ No valid data remaining! Skipping save.")

if __name__ == "__main__":
    main()
